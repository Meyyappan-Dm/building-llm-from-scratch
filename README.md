# ðŸ§  Building a Transformer-based Language Model from Scratch

This project is a minimal, educational implementation of a Transformer-based Language Model (like GPT) using PyTorch â€” built from scratch without relying on high-level libraries.
Inspired by Sebastian Raschkaâ€™s LLM lectures.

## ðŸš€ What I Built

- âœ… Custom tokenizer integration (using `tiktoken`)
- âœ… PyTorch dataset and dataloader with sliding-window tokenization
- âœ… GPT-style Transformer model with:
        - Multi-head self-attention
        - LayerNorm and GELU activation
        - Position embeddings
- âœ… Training loop with train/validation evaluation
- âœ… Text generation from a seed prompt
- âœ… Loss plotting and learning visualization

## Dependencies
      Python 3.8+
      PyTorch
      tiktoken
      matplotlib

## Future Work
     Load and fine-tune pretrained GPT weights
    
     Implement Top-k or nucleus sampling
    
     Add support for multi-GPU training
    
     Improve tokenization using custom BPE or SentencePiece
